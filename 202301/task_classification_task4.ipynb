{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c4c9a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_lcqmc():\n",
    "    '''LCQMC文本匹配数据集\n",
    "    '''\n",
    "    train = pd.read_csv('https://mirror.coggle.club/dataset/LCQMC.train.data.zip', \n",
    "            sep='\\t', names=['query1', 'query2', 'label'])\n",
    "\n",
    "    valid = pd.read_csv('https://mirror.coggle.club/dataset/LCQMC.valid.data.zip', \n",
    "            sep='\\t', names=['query1', 'query2', 'label'])\n",
    "\n",
    "    test = pd.read_csv('https://mirror.coggle.club/dataset/LCQMC.test.data.zip', \n",
    "            sep='\\t', names=['query1', 'query2', 'label'])\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbd27a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5883a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,valid,test = load_lcqmc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50622280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query1</th>\n",
       "      <th>query2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>谁有狂三这张高清的</td>\n",
       "      <td>这张高清图，谁有</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>英雄联盟什么英雄最好</td>\n",
       "      <td>英雄联盟最好英雄是什么</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>这是什么意思，被蹭网吗</td>\n",
       "      <td>我也是醉了，这是什么意思</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>现在有什么动画片好看呢？</td>\n",
       "      <td>现在有什么好看的动画片吗？</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>请问晶达电子厂现在的工资待遇怎么样要求有哪些</td>\n",
       "      <td>三星电子厂工资待遇怎么样啊</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12495</th>\n",
       "      <td>微店怎么开？怎么做代理？</td>\n",
       "      <td>微店怎样代理</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12496</th>\n",
       "      <td>小学科学三年级上</td>\n",
       "      <td>小学三年级科学</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12497</th>\n",
       "      <td>冬眠是什么意思？</td>\n",
       "      <td>冬眠的意思是什么</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12498</th>\n",
       "      <td>天猫有假货吗</td>\n",
       "      <td>天猫卖假货吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12499</th>\n",
       "      <td>天兵天将是什么生肖？</td>\n",
       "      <td>天兵天将是指什么生肖</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       query1         query2  label\n",
       "0                   谁有狂三这张高清的       这张高清图，谁有      0\n",
       "1                  英雄联盟什么英雄最好    英雄联盟最好英雄是什么      1\n",
       "2                 这是什么意思，被蹭网吗   我也是醉了，这是什么意思      0\n",
       "3                现在有什么动画片好看呢？  现在有什么好看的动画片吗？      1\n",
       "4      请问晶达电子厂现在的工资待遇怎么样要求有哪些  三星电子厂工资待遇怎么样啊      0\n",
       "...                       ...            ...    ...\n",
       "12495            微店怎么开？怎么做代理？         微店怎样代理      1\n",
       "12496                小学科学三年级上        小学三年级科学      0\n",
       "12497                冬眠是什么意思？       冬眠的意思是什么      1\n",
       "12498                  天猫有假货吗         天猫卖假货吗      0\n",
       "12499              天兵天将是什么生肖？     天兵天将是指什么生肖      1\n",
       "\n",
       "[12500 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59cd5fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\muma\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.532 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#分词\n",
    "import jieba\n",
    "train_corpus = []\n",
    "valid_corpus = []\n",
    "\n",
    "for i in range(0,len(train)):\n",
    "    query1 = train.iloc[i][\"query1\"]\n",
    "    query2 = train.iloc[i][\"query2\"]\n",
    "    train_corpus.append(jieba.lcut(query1))\n",
    "    train_corpus.append(jieba.lcut(query2))\n",
    "for i in range(0,len(valid)):\n",
    "    query1 = valid.iloc[i][\"query1\"]\n",
    "    query2 = valid.iloc[i][\"query2\"]\n",
    "    valid_corpus.append(jieba.lcut(query1))\n",
    "    valid_corpus.append(jieba.lcut(query2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fa79afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_by_jieba(sentence):\n",
    "    return \" \".join(jieba.lcut(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "557f8aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c669ee58fb401abc1a725bfa909f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3316d11776454d89bc93de3d034a2f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec04d61c8de4b4c8e027a2de35900af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3102b1d86d7648c9bb4ae995ab2e70ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ebbf185eb84188b850f860ec95bb8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbda9b1f0bc4ac49b5221074e40452b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"query1_seg\"] = train[\"query1\"].progress_apply(cut_by_jieba)\n",
    "train[\"query2_seg\"] = train[\"query2\"].progress_apply(cut_by_jieba)\n",
    "valid[\"query1_seg\"] = valid[\"query1\"].progress_apply(cut_by_jieba)\n",
    "valid[\"query2_seg\"] = valid[\"query2\"].progress_apply(cut_by_jieba)\n",
    "test[\"query1_seg\"] = test[\"query1\"].progress_apply(cut_by_jieba)\n",
    "test[\"query2_seg\"] = test[\"query2\"].progress_apply(cut_by_jieba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6165b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练word2vec\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(train_corpus+valid_corpus, vector_size=100, window=5, min_count=5, workers=4)\n",
    "model.save('models//word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac3460e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<495136x5029 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5698648 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#训练tfidf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='char')\n",
    "corpus = []\n",
    "for item in train_corpus:\n",
    "    corpus.append(\" \".join(item))\n",
    "for item in valid_corpus:\n",
    "    corpus.append(\" \".join(item))\n",
    "vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10f51215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "corpus_words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a22994ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39beb43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"models//word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "096d4f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = model.wv.key_to_index\n",
    "We = []\n",
    "for word in words:\n",
    "    We.append(model.wv[word])\n",
    "We = np.array(We)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77ecc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_fq = {}\n",
    "N = 0\n",
    "for datum in train_corpus+valid_corpus:\n",
    "    for seg in datum:\n",
    "        if seg in seg_fq:\n",
    "            seg_fq[seg] += 1\n",
    "        else:\n",
    "            seg_fq[seg] = 1\n",
    "        N += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5437b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordWeight(seg_fq, a=1e-3):\n",
    "    if a <=0: # when the parameter makes no sense, use unweighted\n",
    "        a = 1.0\n",
    "    seg2weight = {}\n",
    "    for key, value in seg_fq.items():\n",
    "        seg2weight[key] = a / (a + value/N)\n",
    "    return seg2weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e2bde64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeight(words, word2weight):\n",
    "    weight4ind = {}\n",
    "    for word, ind in words.items():\n",
    "        if word in word2weight:\n",
    "            weight4ind[ind] = word2weight[word]\n",
    "        else:\n",
    "            weight4ind[ind] = 1.0\n",
    "    return weight4ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca67384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookupIDX(words,w):\n",
    "    w = w.lower()\n",
    "    if len(w) > 1 and w[0] == '#':\n",
    "        w = w.replace(\"#\",\"\")\n",
    "    if w in words:\n",
    "        return words[w]\n",
    "    elif 'UUUNKKK' in words:\n",
    "        return words['UUUNKKK']\n",
    "    else:\n",
    "        return len(words) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c3b73c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeq(p1,words):\n",
    "    p1 = p1.split()\n",
    "    X1 = []\n",
    "    for i in p1:\n",
    "        X1.append(lookupIDX(words,i))\n",
    "    return X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "741f5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(list_of_seqs):\n",
    "    lengths = [len(s) for s in list_of_seqs]\n",
    "    n_samples = len(list_of_seqs)\n",
    "    maxlen = np.max(lengths)\n",
    "    x = np.zeros((n_samples, maxlen)).astype('int32')\n",
    "    x_mask = np.zeros((n_samples, maxlen)).astype('float32')\n",
    "    for idx, s in enumerate(list_of_seqs):\n",
    "        x[idx, :lengths[idx]] = s\n",
    "        x_mask[idx, :lengths[idx]] = 1.\n",
    "    x_mask = np.asarray(x_mask, dtype='float32')\n",
    "    return x, x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a398d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences2idx(sentences, words):\n",
    "    \"\"\"\n",
    "    Given a list of sentences, output array of word indices that can be fed into the algorithms.\n",
    "    :param sentences: a list of sentences\n",
    "    :param words: a dictionary, words['str'] is the indices of the word 'str'\n",
    "    :return: x1, m1. x1[i, :] is the word indices in sentence i, m1[i,:] is the mask for sentence i (0 means no word at the location)\n",
    "    \"\"\"\n",
    "    seq1 = []\n",
    "    for i in sentences:\n",
    "        seq1.append(getSeq(i,words))\n",
    "    x1,m1 = prepare_data(seq1)\n",
    "    return x1, m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73397e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeight(words, seg2weight):\n",
    "    weight4ind = {}\n",
    "    for word, ind in words.items():\n",
    "        if word in seg2weight:\n",
    "            weight4ind[ind] = seg2weight[word]\n",
    "        else:\n",
    "            weight4ind[ind] = 1.0\n",
    "    return weight4ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af64f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2weight(seq, mask, weight4ind):\n",
    "    weight = np.zeros(seq.shape).astype('float32')\n",
    "    for i in range(seq.shape[0]):\n",
    "        for j in range(seq.shape[1]):\n",
    "            if mask[i,j] > 0 and seq[i,j] >= 0:\n",
    "                weight[i,j] = weight4ind[seq[i,j]]\n",
    "    weight = np.asarray(weight, dtype='float32')\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a7b71b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "def get_weighted_average(We, x, w):\n",
    "    \"\"\"\n",
    "    Compute the weighted average vectors\n",
    "    :param We: We[i,:] is the vector for word i\n",
    "    :param x: x[i, :] are the indices of the words in sentence i\n",
    "    :param w: w[i, :] are the weights for the words in sentence i\n",
    "    :return: emb[i, :] are the weighted average vector for sentence i\n",
    "    \"\"\"\n",
    "    n_samples = x.shape[0]\n",
    "    emb = np.zeros((n_samples, We.shape[1]))\n",
    "    for i in range(n_samples):\n",
    "        emb[i,:] = w[i,:].dot(We[x[i,:],:]) / np.count_nonzero(w[i,:])\n",
    "    return emb\n",
    "\n",
    "def compute_pc(X,npc=1):\n",
    "    \"\"\"\n",
    "    Compute the principal components. DO NOT MAKE THE DATA ZERO MEAN!\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: component_[i,:] is the i-th pc\n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n",
    "    svd.fit(X)\n",
    "    return svd.components_\n",
    "\n",
    "def remove_pc(X, npc=1):\n",
    "    \"\"\"\n",
    "    Remove the projection on the principal components\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: XX[i, :] is the data point after removing its projection\n",
    "    \"\"\"\n",
    "    pc = compute_pc(X, npc)\n",
    "    if npc==1:\n",
    "        XX = X - X.dot(pc.transpose()) * pc\n",
    "    else:\n",
    "        XX = X - X.dot(pc.transpose()).dot(pc)\n",
    "    return XX\n",
    "\n",
    "\n",
    "def SIF_embedding(We, x, w, params):\n",
    "    \"\"\"\n",
    "    Compute the scores between pairs of sentences using weighted average + removing the projection on the first principal component\n",
    "    :param We: We[i,:] is the vector for word i\n",
    "    :param x: x[i, :] are the indices of the words in the i-th sentence\n",
    "    :param w: w[i, :] are the weights for the words in the i-th sentence\n",
    "    :param params.rmpc: if >0, remove the projections of the sentence embeddings to their first principal component\n",
    "    :return: emb, emb[i, :] is the embedding for sentence i\n",
    "    \"\"\"\n",
    "    emb = get_weighted_average(We, x, w)\n",
    "#     if  params.rmpc > 0:\n",
    "    emb = remove_pc(emb, params)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa80a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg2weight = getWordWeight(seg_fq, 1e-3)\n",
    "weight4ind = getWeight(words, seg2weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dd15a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算Mean-Pooling\n",
    "def mean_pooling(model, sentence):\n",
    "    embeddings = []\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            embeddings.append(model.wv[word])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros((100,))\n",
    "    else:\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be36445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算Max-Pooling\n",
    "def max_pooling(model, sentence):\n",
    "    embeddings = []\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            embeddings.append(model.wv[word])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros((100,))\n",
    "    else:\n",
    "        return np.max(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ff44804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算tfidf-pooling\n",
    "def tfidf_pooling(model, sentence, vec, vocab):\n",
    "    tfidf = vec.transform([sentence])\n",
    "    embeddings = []\n",
    "    weights = []\n",
    "    for idx, weight in zip(tfidf.indices, tfidf.data):\n",
    "        if idx >= len(vocab):\n",
    "            continue\n",
    "        word = vocab[idx]\n",
    "        try:\n",
    "            embeddings.append(model.wv[word])\n",
    "        except KeyError:\n",
    "            continue\n",
    "        weights.append(weight)\n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros((100,))\n",
    "    return np.average(embeddings, weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ab7c4a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算SIF-pooling\n",
    "def sif_pooing(model,sentence):\n",
    "    x, m = sentences2idx([sentence], words)\n",
    "    w = seq2weight(x, m, weight4ind)\n",
    "    embedding = SIF_embedding(We, x, w, 1)\n",
    "    return embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f495409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = vectorizer.transform([\"喜欢 打篮球 的 男生 喜欢 什么样 的 女生\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "206e7a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3361, 3084, 3003, 2991, 2939, 2386, 2273, 1826, 1223,  997,  383,\n",
       "         323,    0], dtype=int32),\n",
       " array([0.32743866, 0.17890163, 0.20029903, 0.35083583, 0.27177707,\n",
       "        0.45845405, 0.15997743, 0.21489497, 0.18084487, 0.45577642,\n",
       "        0.08932919, 0.07201654, 0.29690745]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.indices, tfidf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1678b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ee8ad9c4524aeb83d5bfc298d742c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ed8b5752de41c39ad15f25b470ba4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7770b77efd48e3bdf260cc62f5abe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe6876741654cda8d562d5f6d4b4d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6eb0bfb09be4473a9e62cb3f12a27c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532cb8bc8a8e47d1ad2d8440d656ee6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['mean_pooling1'] = train['query1_seg'].progress_apply(lambda sentence: mean_pooling(model, sentence))\n",
    "train['mean_pooling2'] = train['query2_seg'].progress_apply(lambda sentence: mean_pooling(model, sentence))\n",
    "valid['mean_pooling1'] = valid['query1_seg'].progress_apply(lambda sentence: mean_pooling(model, sentence))\n",
    "valid['mean_pooling2'] = valid['query2_seg'].progress_apply(lambda sentence: mean_pooling(model, sentence))\n",
    "test['mean_pooling1'] = test['query1_seg'].progress_apply(lambda sentence: mean_pooling(model, sentence))\n",
    "test['mean_pooling2'] = test['query2_seg'].progress_apply(lambda sentence: mean_pooling(model, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "219db461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385dc8e0fde04a13a5208a49b96c3b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ae31061f6240d88c8de935b616de73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20af3e5e0a7644a2b9777ecd58d20cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a350ddcece04f14b99ff60642c2bbaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb45fadbde74017990a0e4e655633a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8433c24da3453e85892e1dbaea6c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['max_pooling1'] = train['query1_seg'].progress_apply(lambda sentence: max_pooling(model, sentence))\n",
    "train['max_pooling2'] = train['query2_seg'].progress_apply(lambda sentence: max_pooling(model, sentence))\n",
    "valid['max_pooling1'] = valid['query1_seg'].progress_apply(lambda sentence: max_pooling(model, sentence))\n",
    "valid['max_pooling2'] = valid['query2_seg'].progress_apply(lambda sentence: max_pooling(model, sentence))\n",
    "test['max_pooling1'] = test['query1_seg'].progress_apply(lambda sentence: max_pooling(model, sentence))\n",
    "test['max_pooling2'] = test['query2_seg'].progress_apply(lambda sentence: max_pooling(model, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02d4f926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c77b3b722c4adb9d0d1a61225b86e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bd6164591f4049a3f45a10787b6cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed84407828f43ba826462c6c9afb313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8c3e46aa52411089e42c047edadb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a97d786b13f49eabf76afd3ef44a392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af74102bb4934a32ad1feb48a8f30419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['tfidf_pooling1'] = train['query1_seg'].progress_apply(lambda sentence: tfidf_pooling(model, sentence, vectorizer, corpus_words))\n",
    "train['tfidf_pooling2'] = train['query2_seg'].progress_apply(lambda sentence: tfidf_pooling(model, sentence, vectorizer, corpus_words))\n",
    "valid['tfidf_pooling1'] = valid['query1_seg'].progress_apply(lambda sentence: tfidf_pooling(model, sentence, vectorizer, corpus_words))\n",
    "valid['tfidf_pooling2'] = valid['query2_seg'].progress_apply(lambda sentence: tfidf_pooling(model, sentence, vectorizer, corpus_words))\n",
    "test['tfidf_pooling1'] = test['query1_seg'].progress_apply(lambda sentence: tfidf_pooling(model, sentence, vectorizer, corpus_words))\n",
    "test['tfidf_pooling2'] = test['query2_seg'].progress_apply(lambda sentence: tfidf_pooling(model, sentence, vectorizer, corpus_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7699ad73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30b6f99562441a38a6275f39de4b766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1c0f6476284502a4c0a4ccefa5adeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238766 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a583ef8b0c8b47b2ab8c198158e7373e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec314d06eb84077810cbc0373762ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bc20b6b02c47c6a82175ec2f930a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe6da73b2a54ab8a7200eb02a6b6da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['sif_pooling1'] = train['query1_seg'].progress_apply(lambda sentence: sif_pooing(model, sentence))\n",
    "train['sif_pooling2'] = train['query2_seg'].progress_apply(lambda sentence: sif_pooing(model, sentence))\n",
    "valid['sif_pooling1'] = valid['query1_seg'].progress_apply(lambda sentence: sif_pooing(model, sentence))\n",
    "valid['sif_pooling2'] = valid['query2_seg'].progress_apply(lambda sentence: sif_pooing(model, sentence))\n",
    "test['sif_pooling1'] = test['query1_seg'].progress_apply(lambda sentence: sif_pooing(model, sentence))\n",
    "test['sif_pooling2'] = test['query2_seg'].progress_apply(lambda sentence: sif_pooing(model, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2da9de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def cos_sim(emb1,emb2):\n",
    "    return 1- cosine(emb1, emb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ebe21d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\envs\\pytorch\\lib\\site-packages\\scipy\\spatial\\distance.py:699: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "#使用test数据进行分类，分类依据是train+valid中label为1和0的比例，即将test的句子对的cos_sim由高到低排序，根据train+valid中1的比例，将test中的样本划分为1和0，再计算accuracy\n",
    "#max_pooling\n",
    "test[\"cos_sim_maxpooling\"] = [cos_sim(emb1,emb2) for emb1,emb2 in zip(test[\"max_pooling1\"],test[\"max_pooling2\"])]\n",
    "#mean_pooling\n",
    "test[\"cos_sim_meanpooling\"] = [cos_sim(emb1,emb2) for emb1,emb2 in zip(test[\"mean_pooling1\"],test[\"mean_pooling2\"])]\n",
    "#tfidf_pooling\n",
    "test[\"cos_sim_tfidfpooling\"] = [cos_sim(emb1,emb2) for emb1,emb2 in zip(test[\"tfidf_pooling1\"],test[\"tfidf_pooling2\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9ed14750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sif_pooling\n",
    "test[\"cos_sim_sifpooling\"] = [cos_sim(emb1,emb2) for emb1,emb2 in zip(test[\"sif_pooling1\"],test[\"sif_pooling2\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7c6cb2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label为1的占比: 0.5775221353325147\n"
     ]
    }
   ],
   "source": [
    "#train+valid中1的比列\n",
    "label_1_percent = (len(train[train[\"label\"] == 1])+len(valid[valid[\"label\"] == 1]))/(len(train)+len(valid))\n",
    "print(\"label为1的占比:\",label_1_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f11c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_maxpooling_sort = test.sort_values(by=\"cos_sim_maxpooling\")\n",
    "cos_sim_meanpooling_sort = test.sort_values(by=\"cos_sim_meanpooling\")\n",
    "cos_sim_tfidfpooling_sort = test.sort_values(by=\"cos_sim_tfidfpooling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "edc6a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_sifpooling_sort = test.sort_values(by=\"cos_sim_sifpooling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1e100ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "通过maxpooling的accuacy:63.408%\n"
     ]
    }
   ],
   "source": [
    "#maxpooling分类效果\n",
    "predict_by_maxpooling_cossim_1 = cos_sim_maxpooling_sort[int(len(test)*(1-label_1_percent)):]\n",
    "predict_by_maxpooling_cossim_0 = cos_sim_maxpooling_sort[:int(len(test)*(1-label_1_percent))]\n",
    "accuracy_from_maxpooling_cossim = 100*(len(predict_by_maxpooling_cossim_1[predict_by_maxpooling_cossim_1[\"label\"] == 1]) + len(predict_by_maxpooling_cossim_0[predict_by_maxpooling_cossim_0[\"label\"] == 0]))/len(test)\n",
    "print(\"通过maxpooling的accuacy:{}%\".format(accuracy_from_maxpooling_cossim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7e9784e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "通过meanpooling的accuacy:66.688%\n"
     ]
    }
   ],
   "source": [
    "#meanpooling分类效果\n",
    "predict_by_meanpooling_cossim_1 = cos_sim_meanpooling_sort[int(len(test)*(1-label_1_percent)):]\n",
    "predict_by_meanpooling_cossim_0 = cos_sim_meanpooling_sort[:int(len(test)*(1-label_1_percent))]\n",
    "accuracy_from_meanpooling_cossim = 100*(len(predict_by_meanpooling_cossim_1[predict_by_meanpooling_cossim_1[\"label\"] == 1]) + len(predict_by_meanpooling_cossim_0[predict_by_meanpooling_cossim_0[\"label\"] == 0]))/len(test)\n",
    "print(\"通过meanpooling的accuacy:{}%\".format(accuracy_from_meanpooling_cossim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6892f7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "通过tfidfpooling的accuacy:68.784%\n"
     ]
    }
   ],
   "source": [
    "#tfidfpooling分类效果\n",
    "predict_by_tfidfpooling_cossim_1 = cos_sim_tfidfpooling_sort[int(len(test)*(1-label_1_percent)):]\n",
    "predict_by_tfidfpooling_cossim_0 = cos_sim_tfidfpooling_sort[:int(len(test)*(1-label_1_percent))]\n",
    "accuracy_from_meanpooling_cossim = 100*(len(predict_by_tfidfpooling_cossim_1[predict_by_tfidfpooling_cossim_1[\"label\"] == 1]) + len(predict_by_tfidfpooling_cossim_0[predict_by_tfidfpooling_cossim_0[\"label\"] == 0]))/len(test)\n",
    "print(\"通过tfidfpooling的accuacy:{}%\".format(accuracy_from_meanpooling_cossim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a4bd82b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "通过sifpooling的accuacy:48.896%\n"
     ]
    }
   ],
   "source": [
    "#sifpooling分类效果\n",
    "predict_by_sifpooling_cossim_1 = cos_sim_sifpooling_sort[int(len(test)*(1-label_1_percent)):]\n",
    "predict_by_sifpooling_cossim_0 = cos_sim_sifpooling_sort[:int(len(test)*(1-label_1_percent))]\n",
    "accuracy_from_meanpooling_cossim = 100*(len(predict_by_sifpooling_cossim_1[predict_by_sifpooling_cossim_1[\"label\"] == 1]) + len(predict_by_sifpooling_cossim_0[predict_by_sifpooling_cossim_0[\"label\"] == 0]))/len(test)\n",
    "print(\"通过sifpooling的accuacy:{}%\".format(accuracy_from_meanpooling_cossim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d0ce18e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SIF效果很差，应该有问题，后面需要再进行检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ad3f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
