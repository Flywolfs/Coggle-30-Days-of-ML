{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bd27528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_lcqmc():\n",
    "    '''LCQMC文本匹配数据集\n",
    "    '''\n",
    "    train = pd.read_csv('https://mirror.coggle.club/dataset/LCQMC.train.data.zip', \n",
    "            sep='\\t', names=['query1', 'query2', 'label'])\n",
    "\n",
    "    valid = pd.read_csv('https://mirror.coggle.club/dataset/LCQMC.valid.data.zip', \n",
    "            sep='\\t', names=['query1', 'query2', 'label'])\n",
    "\n",
    "    test = pd.read_csv('https://mirror.coggle.club/dataset/LCQMC.test.data.zip', \n",
    "            sep='\\t', names=['query1', 'query2', 'label'])\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a95fd94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from loguru import logger\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import spearmanr\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import BertConfig, BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c530f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e70b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,valid,test = load_lcqmc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e4fb4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query1    我手机丢了，我想换个手机\n",
       "query2     我想买个新手机，求推荐\n",
       "label                1\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0d0a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df):\n",
    "    return [(df.iloc[i][\"query1\"],df.iloc[i][\"query2\"],df.iloc[i][\"label\"]) for i in range(0,len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "091bc65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(train)\n",
    "valid_data = load_data(valid)\n",
    "test_data = load_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e020a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "SAMPLES = 10000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "LR = 1e-5\n",
    "DROPOUT = 0.3\n",
    "MAXLEN = 64\n",
    "POOLING = 'cls'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_data = random.sample(train_data, SAMPLES) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3462f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    \"\"\"训练数据集, 重写__getitem__和__len__方法\"\"\"\n",
    "    def __init__(self, data: List):\n",
    "        self.data = data\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def text_2_id(self, text: str):\n",
    "        # 添加自身两次, 经过bert编码之后, 互为正样本\n",
    "        return tokenizer([text, text], max_length=MAXLEN, truncation=True, padding='max_length', return_tensors='pt')\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        return self.text_2_id(self.data[index][0])\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    \"\"\"测试数据集, 重写__getitem__和__len__方法\"\"\"\n",
    "    def __init__(self, data: List):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def text_2_id(self, text: str):\n",
    "        return tokenizer(text, max_length=MAXLEN, truncation=True, padding='max_length', return_tensors='pt')\n",
    "    \n",
    "    def __getitem__(self, index: int):        \n",
    "        return self.text_2_id([self.data[index][0]]), self.text_2_id([self.data[index][1]]), int(self.data[index][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c70458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(TrainDataset(train_data), batch_size=BATCH_SIZE)\n",
    "valid_dataloader = DataLoader(TestDataset(valid_data), batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(TestDataset(test_data), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0a8f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimcseModel(nn.Module):\n",
    "    \"\"\"Simcse无监督模型定义\"\"\"\n",
    "    def __init__(self, pretrained_model, pooling):\n",
    "        super(SimcseModel, self).__init__()\n",
    "        config = BertConfig.from_pretrained(pretrained_model)       \n",
    "        config.attention_probs_dropout_prob = DROPOUT   # 修改config的dropout系数\n",
    "        config.hidden_dropout_prob = DROPOUT           \n",
    "        self.bert = BertModel.from_pretrained(pretrained_model, config=config)\n",
    "        self.pooling = pooling\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "\n",
    "        out = self.bert(input_ids, attention_mask, token_type_ids, output_hidden_states=True)\n",
    "\n",
    "        if self.pooling == 'cls':\n",
    "            return out.last_hidden_state[:, 0]  # [batch, 768]\n",
    "        \n",
    "        if self.pooling == 'pooler':\n",
    "            return out.pooler_output            # [batch, 768]\n",
    "        \n",
    "        if self.pooling == 'last-avg':\n",
    "            last = out.last_hidden_state.transpose(1, 2)    # [batch, 768, seqlen]\n",
    "            return torch.avg_pool1d(last, kernel_size=last.shape[-1]).squeeze(-1)       # [batch, 768]\n",
    "        \n",
    "        if self.pooling == 'first-last-avg':\n",
    "            first = out.hidden_states[1].transpose(1, 2)    # [batch, 768, seqlen]\n",
    "            last = out.hidden_states[-1].transpose(1, 2)    # [batch, 768, seqlen]                   \n",
    "            first_avg = torch.avg_pool1d(first, kernel_size=last.shape[-1]).squeeze(-1) # [batch, 768]\n",
    "            last_avg = torch.avg_pool1d(last, kernel_size=last.shape[-1]).squeeze(-1)   # [batch, 768]\n",
    "            avg = torch.cat((first_avg.unsqueeze(1), last_avg.unsqueeze(1)), dim=1)     # [batch, 2, 768]\n",
    "            return torch.avg_pool1d(avg.transpose(1, 2), kernel_size=2).squeeze(-1)     # [batch, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94560f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simcse_unsup_loss(y_pred: 'tensor') -> 'tensor':\n",
    "    \"\"\"无监督的损失函数\n",
    "    y_pred (tensor): bert的输出, [batch_size * 2, 768]\n",
    "    \n",
    "    \"\"\"\n",
    "    # 得到y_pred对应的label, [1, 0, 3, 2, ..., batch_size-1, batch_size-2]\n",
    "    y_true = torch.arange(y_pred.shape[0], device=DEVICE)\n",
    "    y_true = (y_true - y_true % 2 * 2) + 1\n",
    "    # batch内两两计算相似度, 得到相似度矩阵(对角矩阵)\n",
    "    sim = F.cosine_similarity(y_pred.unsqueeze(1), y_pred.unsqueeze(0), dim=-1)\n",
    "    # 将相似度矩阵对角线置为很小的值, 消除自身的影响\n",
    "    sim = sim - torch.eye(y_pred.shape[0], device=DEVICE) * 1e12\n",
    "    # 相似度矩阵除以温度系数\n",
    "    sim = sim / 0.05\n",
    "    # 计算相似度矩阵与y_true的交叉熵损失\n",
    "    loss = F.cross_entropy(sim, y_true)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee63b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader) -> float:\n",
    "    \"\"\"模型评估函数 \n",
    "    批量预测, batch结果拼接, 一次性求spearman相关度\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    sim_tensor = torch.tensor([], device=DEVICE)\n",
    "    label_array = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for source, target, label in dataloader:\n",
    "            # source        [batch, 1, seq_len] -> [batch, seq_len]\n",
    "            source_input_ids = source.get('input_ids').squeeze(1).to(DEVICE)\n",
    "            source_attention_mask = source.get('attention_mask').squeeze(1).to(DEVICE)\n",
    "            source_token_type_ids = source.get('token_type_ids').squeeze(1).to(DEVICE)\n",
    "            source_pred = model(source_input_ids, source_attention_mask, source_token_type_ids)\n",
    "            # target        [batch, 1, seq_len] -> [batch, seq_len]\n",
    "            target_input_ids = target.get('input_ids').squeeze(1).to(DEVICE)\n",
    "            target_attention_mask = target.get('attention_mask').squeeze(1).to(DEVICE)\n",
    "            target_token_type_ids = target.get('token_type_ids').squeeze(1).to(DEVICE)\n",
    "            target_pred = model(target_input_ids, target_attention_mask, target_token_type_ids)\n",
    "            # concat\n",
    "            sim = F.cosine_similarity(source_pred, target_pred, dim=-1)\n",
    "            sim_tensor = torch.cat((sim_tensor, sim), dim=0)            \n",
    "            label_array = np.append(label_array, np.array(label))\n",
    "    # corrcoef \n",
    "    return spearmanr(label_array, sim_tensor.cpu().numpy()).correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1ef8b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl, dev_dl, optimizer) -> None:\n",
    "    \"\"\"模型训练函数\"\"\"\n",
    "    model.train()\n",
    "    global best\n",
    "    for batch_idx, source in enumerate(tqdm(train_dl), start=1):\n",
    "        # 维度转换 [batch, 2, seq_len] -> [batch * 2, sql_len]\n",
    "        real_batch_num = source.get('input_ids').shape[0]\n",
    "        input_ids = source.get('input_ids').view(real_batch_num * 2, -1).to(DEVICE)\n",
    "        attention_mask = source.get('attention_mask').view(real_batch_num * 2, -1).to(DEVICE)\n",
    "        token_type_ids = source.get('token_type_ids').view(real_batch_num * 2, -1).to(DEVICE)\n",
    "        \n",
    "        out = model(input_ids, attention_mask, token_type_ids)        \n",
    "        loss = simcse_unsup_loss(out)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:     \n",
    "            logger.info(f'loss: {loss.item():.4f}')\n",
    "            corrcoef = eval(model, dev_dl)\n",
    "            model.train()\n",
    "            if best < corrcoef:\n",
    "                best = corrcoef\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                logger.info(f\"higher corrcoef: {best:.4f} in batch: {batch_idx}, save model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "32f10bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'G:\\\\deep_learning\\\\models\\\\chinese_wwm_ext_pytorch'\n",
    "save_path = 'G:\\\\deep_learning\\\\Coggle\\\\202301\\\\models\\\\simcse\\\\chinese_bert_wwm_ext\\\\simcse_unsup.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd3029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 23:03:34.144 | INFO     | __main__:<module>:1 - device: cuda, pooling: cls, model path: G:\\deep_learning\\models\\chinese_wwm_ext_pytorch\n",
      "Some weights of the model checkpoint at G:\\deep_learning\\models\\chinese_wwm_ext_pytorch were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-01-30 23:03:35.038 | INFO     | __main__:<module>:10 - epoch: 0\n",
      "  3%|██▎                                                                               | 9/313 [00:02<01:15,  4.03it/s]2023-01-30 23:03:37.599 | INFO     | __main__:train:19 - loss: 1.6295\n",
      "2023-01-30 23:04:00.902 | INFO     | __main__:train:25 - higher corrcoef: 0.5173 in batch: 10, save model\n",
      "  6%|████▉                                                                            | 19/313 [00:28<02:34,  1.90it/s]2023-01-30 23:04:03.313 | INFO     | __main__:train:19 - loss: 0.7066\n",
      "2023-01-30 23:04:26.893 | INFO     | __main__:train:25 - higher corrcoef: 0.5630 in batch: 20, save model\n",
      "  9%|███████▌                                                                         | 29/313 [00:54<02:36,  1.81it/s]2023-01-30 23:04:29.476 | INFO     | __main__:train:19 - loss: 0.4814\n",
      "2023-01-30 23:04:53.217 | INFO     | __main__:train:25 - higher corrcoef: 0.5813 in batch: 30, save model\n",
      " 12%|██████████                                                                       | 39/313 [01:20<02:28,  1.84it/s]2023-01-30 23:04:55.686 | INFO     | __main__:train:19 - loss: 0.3265\n",
      "2023-01-30 23:05:19.549 | INFO     | __main__:train:25 - higher corrcoef: 0.5919 in batch: 40, save model\n",
      " 16%|████████████▋                                                                    | 49/313 [01:46<02:22,  1.85it/s]2023-01-30 23:05:22.012 | INFO     | __main__:train:19 - loss: 0.3145\n",
      "2023-01-30 23:05:45.248 | INFO     | __main__:train:25 - higher corrcoef: 0.5990 in batch: 50, save model\n",
      " 19%|███████████████▎                                                                 | 59/313 [02:12<02:15,  1.88it/s]2023-01-30 23:05:47.667 | INFO     | __main__:train:19 - loss: 0.2701\n",
      "2023-01-30 23:06:10.408 | INFO     | __main__:train:25 - higher corrcoef: 0.6039 in batch: 60, save model\n",
      " 22%|█████████████████▊                                                               | 69/313 [02:37<02:08,  1.90it/s]2023-01-30 23:06:12.836 | INFO     | __main__:train:19 - loss: 0.2315\n",
      "2023-01-30 23:06:35.627 | INFO     | __main__:train:25 - higher corrcoef: 0.6055 in batch: 70, save model\n",
      " 25%|████████████████████▍                                                            | 79/313 [03:02<02:03,  1.90it/s]2023-01-30 23:06:38.053 | INFO     | __main__:train:19 - loss: 0.1563\n",
      "2023-01-30 23:07:00.928 | INFO     | __main__:train:25 - higher corrcoef: 0.6080 in batch: 80, save model\n",
      " 28%|███████████████████████                                                          | 89/313 [03:28<01:58,  1.89it/s]2023-01-30 23:07:03.364 | INFO     | __main__:train:19 - loss: 0.2126\n",
      "2023-01-30 23:07:26.136 | INFO     | __main__:train:25 - higher corrcoef: 0.6099 in batch: 90, save model\n",
      " 32%|█████████████████████████▌                                                       | 99/313 [03:53<01:52,  1.90it/s]2023-01-30 23:07:28.593 | INFO     | __main__:train:19 - loss: 0.2481\n",
      "2023-01-30 23:07:51.328 | INFO     | __main__:train:25 - higher corrcoef: 0.6119 in batch: 100, save model\n",
      " 35%|███████████████████████████▊                                                    | 109/313 [04:18<01:47,  1.90it/s]2023-01-30 23:07:53.766 | INFO     | __main__:train:19 - loss: 0.0861\n",
      "2023-01-30 23:08:16.674 | INFO     | __main__:train:25 - higher corrcoef: 0.6120 in batch: 110, save model\n",
      " 38%|██████████████████████████████▍                                                 | 119/313 [04:43<01:42,  1.89it/s]2023-01-30 23:08:19.100 | INFO     | __main__:train:19 - loss: 0.1310\n",
      "2023-01-30 23:08:42.108 | INFO     | __main__:train:25 - higher corrcoef: 0.6130 in batch: 120, save model\n",
      " 41%|████████████████████████████████▉                                               | 129/313 [05:09<01:37,  1.89it/s]2023-01-30 23:08:44.536 | INFO     | __main__:train:19 - loss: 0.2041\n",
      "2023-01-30 23:09:07.419 | INFO     | __main__:train:25 - higher corrcoef: 0.6151 in batch: 130, save model\n",
      " 44%|███████████████████████████████████▌                                            | 139/313 [05:34<01:31,  1.90it/s]2023-01-30 23:09:09.842 | INFO     | __main__:train:19 - loss: 0.2031\n",
      "2023-01-30 23:09:32.704 | INFO     | __main__:train:25 - higher corrcoef: 0.6173 in batch: 140, save model\n",
      " 48%|██████████████████████████████████████                                          | 149/313 [05:59<01:26,  1.89it/s]2023-01-30 23:09:35.146 | INFO     | __main__:train:19 - loss: 0.1163\n",
      "2023-01-30 23:09:57.958 | INFO     | __main__:train:25 - higher corrcoef: 0.6195 in batch: 150, save model\n",
      " 51%|████████████████████████████████████████▋                                       | 159/313 [06:25<01:23,  1.85it/s]2023-01-30 23:10:00.549 | INFO     | __main__:train:19 - loss: 0.1129\n",
      "2023-01-30 23:10:25.404 | INFO     | __main__:train:25 - higher corrcoef: 0.6208 in batch: 160, save model\n",
      " 54%|███████████████████████████████████████████▏                                    | 169/313 [06:52<01:19,  1.81it/s]2023-01-30 23:10:27.836 | INFO     | __main__:train:19 - loss: 0.1820\n",
      "2023-01-30 23:10:50.755 | INFO     | __main__:train:25 - higher corrcoef: 0.6216 in batch: 170, save model\n",
      " 57%|█████████████████████████████████████████████▊                                  | 179/313 [07:17<01:10,  1.89it/s]2023-01-30 23:10:53.181 | INFO     | __main__:train:19 - loss: 0.0795\n",
      "2023-01-30 23:11:16.069 | INFO     | __main__:train:25 - higher corrcoef: 0.6226 in batch: 180, save model\n",
      " 60%|████████████████████████████████████████████████▎                               | 189/313 [07:43<01:05,  1.89it/s]2023-01-30 23:11:18.512 | INFO     | __main__:train:19 - loss: 0.0579\n",
      " 64%|██████████████████████████████████████████████████▊                             | 199/313 [08:07<00:59,  1.93it/s]2023-01-30 23:11:42.930 | INFO     | __main__:train:19 - loss: 0.0478\n",
      " 67%|█████████████████████████████████████████████████████▍                          | 209/313 [08:32<00:54,  1.93it/s]2023-01-30 23:12:07.527 | INFO     | __main__:train:19 - loss: 0.1055\n",
      " 70%|███████████████████████████████████████████████████████▉                        | 219/313 [08:56<00:48,  1.93it/s]2023-01-30 23:12:31.885 | INFO     | __main__:train:19 - loss: 0.1256\n",
      " 73%|██████████████████████████████████████████████████████████▌                     | 229/313 [09:20<00:43,  1.93it/s]2023-01-30 23:12:56.248 | INFO     | __main__:train:19 - loss: 0.0545\n",
      " 76%|█████████████████████████████████████████████████████████████                   | 239/313 [09:45<00:38,  1.94it/s]2023-01-30 23:13:20.588 | INFO     | __main__:train:19 - loss: 0.1090\n",
      " 80%|███████████████████████████████████████████████████████████████▋                | 249/313 [10:09<00:33,  1.93it/s]2023-01-30 23:13:45.025 | INFO     | __main__:train:19 - loss: 0.0989\n",
      "2023-01-30 23:14:08.821 | INFO     | __main__:train:25 - higher corrcoef: 0.6233 in batch: 250, save model\n",
      " 83%|██████████████████████████████████████████████████████████████████▏             | 259/313 [10:35<00:29,  1.85it/s]2023-01-30 23:14:11.275 | INFO     | __main__:train:19 - loss: 0.0643\n",
      "2023-01-30 23:14:34.405 | INFO     | __main__:train:25 - higher corrcoef: 0.6256 in batch: 260, save model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████████████████████████████████████▊           | 269/313 [11:01<00:23,  1.87it/s]2023-01-30 23:14:36.855 | INFO     | __main__:train:19 - loss: 0.0275\n",
      "2023-01-30 23:15:01.358 | INFO     | __main__:train:25 - higher corrcoef: 0.6266 in batch: 270, save model\n",
      " 89%|███████████████████████████████████████████████████████████████████████▎        | 279/313 [11:28<00:18,  1.80it/s]2023-01-30 23:15:03.902 | INFO     | __main__:train:19 - loss: 0.0474\n",
      " 92%|█████████████████████████████████████████████████████████████████████████▊      | 289/313 [11:54<00:12,  1.86it/s]2023-01-30 23:15:29.839 | INFO     | __main__:train:19 - loss: 0.0469\n",
      "2023-01-30 23:15:52.829 | INFO     | __main__:train:25 - higher corrcoef: 0.6273 in batch: 290, save model\n",
      " 96%|████████████████████████████████████████████████████████████████████████████▍   | 299/313 [12:19<00:07,  1.88it/s]2023-01-30 23:15:55.274 | INFO     | __main__:train:19 - loss: 0.0816\n",
      " 99%|██████████████████████████████████████████████████████████████████████████████▉ | 309/313 [12:44<00:02,  1.93it/s]2023-01-30 23:16:19.669 | INFO     | __main__:train:19 - loss: 0.0408\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 313/313 [13:07<00:00,  2.52s/it]\n",
      "2023-01-30 23:16:42.675 | INFO     | __main__:<module>:12 - train is finished, best model is saved at G:\\deep_learning\\Coggle\\202301\\models\\simcse\\chinese_bert_wwm_ext\\simcse_unsup.pt\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'device: {DEVICE}, pooling: {POOLING}, model path: {model_path}')\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "# load model\n",
    "assert POOLING in ['cls', 'pooler', 'last-avg', 'first-last-avg']\n",
    "model = SimcseModel(pretrained_model=model_path, pooling=POOLING).to(DEVICE)  \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "# train\n",
    "best = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    logger.info(f'epoch: {epoch}')\n",
    "    train(model, train_dataloader, valid_dataloader, optimizer)\n",
    "logger.info(f'train is finished, best model is saved at {save_path}')\n",
    "# eval\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "valid_corrcoef = eval(model, valid_dataloader)\n",
    "test_corrcoef = eval(model, test_dataloader)\n",
    "logger.info(f'dev_corrcoef: {valid_corrcoef:.4f}')\n",
    "logger.info(f'test_corrcoef: {test_corrcoef:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38932077",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
